大規模言語モデル（LLM：Large language Models）とは、大量のデータとディープラーニング（深層学習）技術によって構築された言語モデルです。言語モデルは文章や単語の出現確率を用いてモデル化したものであり、文章作成などの自然言語処理で用いられています。大規模言語モデルと従来の言語モデルでは、「データ量」「計算量」「パラメータ量」が大きく異なります。

データ量：入力される情報量
計算量：コンピューターが処理する計算量
パラメータ量：確率計算を行うための係数量
大規模言語モデルでは上記の3点が大幅に増加したことで、精度が格段に向上しました。大規模言語モデルは、2017年に発表された「Transformer」がきっかけとなって構築されたといわれています。Transformerの登場によるブレイクスルーと、マシンパワーの向上によって、より多くのデータをモデルに学習させられるようになったことで大規模言語モデルは誕生しました。

大規模言語モデルは、おおまかに表すと次のような仕組みで構築されています。

トークン化：入力文を最小単位に分別
文脈理解：プロンプト内の各トークンとの関連性を計算
エンコード：特徴量の抽出
デコード：次のトークンを予測
入力文の次のトークンの確率を出力
大規模言語モデルは基本的にTransformerの仕組みを利用しています。従来の言語モデルでは、テキストデータであれば単語に分割した後に人がラベル付けをする必要がありました。しかし、大規模言語モデルでは大量のテキストデータを与えることで、トークンから文脈や言葉の意味を学習できます。この学習した結果から、特定の言葉に続く確率が高いと考えられる言葉・文章を並べられるものが大規模言語モデルなのです。

大規模言語モデルの礎となったTransformerをもとに開発された有名な大規模言語モデルを紹介します。

BERT（Bidirectional Encoder Representations from Transformesrs）は、2018年にGoogleの論文で発表された自然言語処理モデルです。日本語では「Transformerによる双方向のエンコード表現」と訳されます。BERTは文章を文頭と文末（双方向）から学習することで「文脈を読める」ようになりました。翻訳や質問応答などの自然言語処理タスクにおいて、2018年当時では最高スコアを記録しています。

「ChatGPT」として多くの方が認知しているサービスでは、GPT（Generative Pre-trained Transformer）と呼ばれる大規模言語モデルが利用されています。GPTは大量のテキストデータを事前学習した後に、特定のタスクに適用させるファインチューニングと呼ばれる学習をする2段階の学習モデルです。
GPT-3はOpenAI社によって2020年に発表され、ChatGPTではチャット向けにファインチューニングしたGPT-3.5が利用されています。

GPT-4は2023年にアップデートされたGPTの最新版であり、テキストだけでなく画像などの入力を受け取ってテキストを出力できる「マルチモーダル」なモデルです。GPT-3.5で扱えるトークンの最大数は4,097であったのに対し、GPT-4では3万2,768トークンと約8倍に増えています。そのため、GPT-3.5よりも複雑な質問にも回答できるようになりました。

大規模言語モデルは、人間のような自然な受け答えや文章の作成が可能であることから、次のような用途で活用されています。

カスタマーサポート
文章の作成
文章の校正
リアルタイムの翻訳
プログラムのバグチェック
など

また、GPT-4のようにテキストデータからだけでなく、画像や音声データなど学習した大規模言語モデルも登場しており、今後はさらにさまざまな用途への活用が期待されています。

大規模言語モデルは、大量のデータとディープラーニング技術によって構築された言語モデルです。近年では大規模言語モデルを利用した生成系AIが、積極的に活用されています。そのなかでも特に利用されることの多いものとしては「ChatGPT」が挙げられるでしょう。ChatGPTについてより詳しく知りたい方は、こちらの記事も併せてご覧ください。
ChatGPTとは？ 始め方や賢い活用方法などわかりやすく解説

また、人材不足をはじめとする企業が抱える多くの課題を解決する手段としてもAIは注目されています。日立ソリューションズ・クリエイトでは、既存のシステムにAIをプラスすることで、お客さまの課題解決を支援する「AIプラス」を提供しています。DX推進・業務効率化・人材不足などへの対策を検討されている場合は、ぜひ一度ご相談ください。